# -*- coding: utf-8 -*-
"""deepseek-r1-finetuned-news_summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1izzt1M7yBN756RjZSbLGntE00TZOARc7
"""

!pip install transformers datasets evaluate rouge-score peft trl accelerate bitsandbytes huggingface-hub

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training
)
from trl import SFTTrainer, SFTConfig
from evaluate import load
import numpy as np
from huggingface_hub import HfFolder
import torch

# Set your Hugging Face token
HF_TOKEN = ""  # Replace with your token
MODEL_NAME = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
OUTPUT_MODEL_NAME = "xiddiqui/deepseek-summarizer"  # Replace with your desired model name

def compute_metrics(eval_preds):
    rouge = load("rouge")

    predictions, labels = eval_preds
    # SFTTrainer outputs raw logits, need to get predicted token ids
    predictions = predictions.argmax(axis=-1)

    # Decode predictions and labels
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ROUGE expects a newline after each sentence
    decoded_preds = ["\n".join(pred.split()) for pred in decoded_preds]
    decoded_labels = ["\n".join(label.split()) for label in decoded_labels]

    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    return {k: round(v * 100, 4) for k, v in result.items()}

def prepare_dataset():
    # Load a smaller subset of the dataset for faster training
    dataset = load_dataset("cnn_dailymail", "3.0.0")

    # Take a smaller subset for training
    train_dataset = dataset["train"].select(range(1000))  # Adjust size as needed
    val_dataset = dataset["validation"].select(range(100))

    return train_dataset, val_dataset

def format_prompt(example):
    return f"""Summarize the following article:

    Article: {example['article']}

    Summary: {example['highlights']}"""

def preprocess_function(examples):
    """
    Preprocesses the dataset by formatting and adding a 'text' field.
    This ensures compatibility with SFTTrainer's dataset field requirement.
    """
    prompts = [
        format_prompt({"article": article, "highlights": summary})
        for article, summary in zip(examples["article"], examples["highlights"])
    ]
    return {"text": prompts}

HfFolder.save_token(HF_TOKEN)

# Configure quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

# Load base model with quantization
global tokenizer  # Make tokenizer accessible to preprocess_function
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

# Configure LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Prepare dataset
train_dataset, val_dataset = prepare_dataset()

# Preprocess datasets
tokenized_train = train_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=train_dataset.column_names
)
tokenized_val = val_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=val_dataset.column_names
)

from trl import SFTConfig

# SFT Configuration
sft_config = SFTConfig(
    dataset_text_field="text",
    max_seq_length=512,
    output_dir="./results",
    logging_dir="./logs",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    logging_steps=10,
    save_steps=500,
    save_total_limit=3,
    report_to="tensorboard",
    fp16=True,
)

# Initialize SFT trainer
trainer = SFTTrainer(
    model=model,
    args=sft_config,
    tokenizer=tokenizer,
    train_dataset=tokenized_train,
    eval_dataset = tokenized_val,
    compute_metrics=compute_metrics,
    peft_config = lora_config
)

trainer.train()

# Save the LoRA adapter
trainer.model.save_pretrained("./lora_adapter")
tokenizer.save_pretrained("./lora_adapter")

# Save the LoRA adapter

trainer.model.save_pretrained("drive/MyDrive/lora_adapter")
tokenizer.save_pretrained("drive/MyDrive/lora_adapter")

def format_prompt2(article):
    """
    Formats the input article into the prompt format used during training.
    """
    return f"""Summarize the following article:

    Article: {article}

    Summary:"""

# Example input
article = """As they rounded a bend in the path that ran beside the river, Lara recognized the silhouette of a fig tree atop a nearby hill. The weather was hot and the days were long. The fig tree was in full leaf, but not yet bearing fruit.
Soon Lara spotted other landmarks—an outcropping of limestone beside the path that had a silhouette like a man’s face, a marshy spot beside the river where the waterfowl were easily startled, a tall tree that looked like a man with his arms upraised. They were drawing near to the place where there was an island in the river. The island was a good spot to make camp. They would sleep on the island tonight.
Lara had been back and forth along the river path many times in her short life. Her people had not created the path—it had always been there, like the river—but their deerskin-shod feet and the wooden wheels of their handcarts kept the path well worn. Lara’s people were salt traders, and their livelihood took them on a continual journey.
At the mouth of the river, the little group of half a dozen intermingled families gathered salt from the great salt beds beside the sea. They groomed and sifted the salt and loaded it into handcarts. When the carts were full, most of the group would stay behind, taking shelter amid rocks and simple lean-tos, while a band of fifteen or so of the heartier members set out on the path that ran alongside the river.
With their precious cargo of salt, the travelers crossed the coastal lowlands and traveled toward the mountains. But Lara’s people never reached the mountaintops; they traveled only as far as the foothills. Many people lived in the forests and grassy meadows of the foothills, gathered in small villages. In return for salt, these people would give Lara’s people dried meat, animal skins, cloth spun from wool, clay pots, needles and scraping tools carved from bone, and little toys made of wood.
"""
formatted_prompt = format_prompt2(article)

print(formatted_prompt)

# Check if CUDA is available and move the model to GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Tokenize the input and move it to the same device
inputs = tokenizer(
    formatted_prompt,
    return_tensors="pt",
    truncation=True,
    max_length=512,
    padding=True
).to(device)

# Generate the summary
outputs = model.generate(
    input_ids=inputs["input_ids"],
    num_beams=5,     # Beam search for better quality
    early_stopping=True
)

# Decode and print the summary
summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated Summary:", summary)



len(summary)

# Clear CUDA cache
torch.cuda.empty_cache()

print("Loading base model...")
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    low_cpu_mem_usage=True
)

print("Loading LoRA adapter...")
model = PeftModel.from_pretrained(
    base_model,
    "./lora_adapter",
    device_map="auto"
)

print("Merging weights...")
merged_model = model.merge_and_unload()

print("Saving merged model...")
merged_model.save_pretrained(
    "./merged_model",
    safe_serialization=True,
    max_shard_size="500MB"  # Split into smaller files
)

print("Pushing to Hub...")
merged_model.push_to_hub(OUTPUT_MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.push_to_hub(OUTPUT_MODEL_NAME)

print("Merge and push completed!")